import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Subset, DataLoader
import itertools
import random
import numpy as np


def split_dataset(dataset, num_parties, batch_size, shuffle=True):
    """
    Split a given dataset into a number of parties, each containing a part of the dataset.
    Create data loaders for each party.

    Args:
        dataset (torch.utils.data.Dataset): The dataset to be split.
        num_parties (int): The number of parties to split the dataset into.
        batch_size (int): The batch size for each party's data loader.
        shuffle (bool, optional): Whether to shuffle the data indices before splitting.
                                  Defaults to True.

    Returns:
        list: A list of data loaders, with each data loader containing a subset of the dataset
              assigned to a specific party.
    """
    dataset_size = len(dataset)
    data_indices = list(range(dataset_size))
    split_size = dataset_size // num_parties

    if shuffle:
        random.shuffle(data_indices)

    party_data_indices = [data_indices[i:i+split_size] for i in range(0, dataset_size, split_size)]
    party_dataloaders = []

    for indices in party_data_indices:
        party_dataset = Subset(dataset, indices)
        party_dataloader = DataLoader(party_dataset, batch_size=batch_size, shuffle=shuffle)
        party_dataloaders.append(party_dataloader)

    return party_dataloaders

def test_ensemble_model(model_list, test_loader):
    """
    Evaluate an ensemble of deep learning models on a test dataset and calculate the accuracy.

    Args:
        model_list (list): A list of PyTorch models to form the ensemble.
        test_loader (torch.utils.data.DataLoader): The data loader for the test dataset.

    Returns:
        float: The accuracy of the ensemble model on the test dataset.

    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    for model in model_list:
        model.eval()
        model.to(device)

    correct = 0
    total = 0

    with torch.no_grad():
        for data, targets in test_loader:
            #The target should be a LongTensor using nn.CrossEntropyLoss (or nn.NLLLoss)
            target = targets.long()
            images, labels = data.to(device), targets.to(device)
            predictions = []

            for model in model_list:
                outputs = model(images)
                probabilities = F.softmax(outputs, dim=1)
                predictions.append(probabilities)

            ensemble_predictions = torch.stack(predictions).mean(dim=0)
            _, predicted = torch.max(ensemble_predictions.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f"Ensemble Accuracy: {accuracy:.2f}%")
    return accuracy

def client_update(model, optimizer, train_loader, num_epochs):
    """
    Perform a local training update for a client in a federated learning system.

    Args:
        model (torch.nn.Module): The client model to be trained.
        optimizer (torch.optim.Optimizer): The optimizer used for training.
        learning_rate (float): step learning rate
        train_loader (torch.utils.data.DataLoader): The data loader for the client's training dataset.
        num_epochs (int): The number of training epochs.

    Returns:
        torch.nn.Module: The updated client model.

    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = torch.nn.functional.cross_entropy(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        epoch_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}")

    return model

def aggregate_models(models, global_model, num_clients):
    """
    Aggregate models from multiple clients using Federated Averaging (FedAvg) algorithm.

    Args:
        models ([Model]): List of client models to be aggregated.
        global_model (Model): Aggregator model to update
        num_clients (int): number of participating clients in the system

    Returns:
        torch.nn.Module: Aggregated global model.

    """
    global_dict = global_model.state_dict()

    for key in global_dict.keys():
        update_dict = torch.stack([(model.state_dict()[key].float() - global_dict[key].float()) for model in models], 0).sum(dim=0)
        global_dict[key] = global_dict[key] + update_dict * (1/num_clients)
    global_model.load_state_dict(global_dict)
    return global_model

def create_permutation_matrix(q, k):
    """
    Create a permutation matrix of size q x k.

    Args:
        q (int): The number of strata.
        k (int): The number of communication rounds in an age (equal to number of aggregators).

    Returns:
        numpy.ndarray: Permutation matrix of size q x k.

    """
    perm_matrix = np.zeros((q, k))

    for i in range(q):
        perm_matrix[i] = np.random.permutation(k)

    perm_matrix = perm_matrix.astype(int).tolist()
    return perm_matrix

def fed_ensemble(C, T, A, P, test_dataloader, learning_rate, num_epochs, Q):
    """
    Implement the Fed-Ensemble algorithm for federated learning.

    Args:
        num_epochs (int): The number of training epochs for each round.
        T (int): The number of ages for the Fed-Ensemble algorithm.
        C ([dataloader]): Local dataset of parties (clients) in the federated learning setup.
        A ([Model]): List with aggregator models.
        P (int): The number of participants selected per round.
        test_dataloader (dataloader): Dataset for validation between rounds.
        learning_rate (float): Learning rate for clients.
        num_epochs (int): number of epochs for local training
        Q (int): amount of strata in which to divide the clients

    Returns:
        list: The global models trained using the Fed-Ensemble algorithm.

    """
    # create optimizers for the models
    opt = [optim.SGD(model.parameters(model), lr=learning_rate) for model in A]
    # Randomly divide N clients into Q strata
    clients_per_stratum = len(C) // Q
    strata = [C[i:i+clients_per_stratum] for i in range(0, len(C), clients_per_stratum)]
    print('strata', len(strata[0]), len(strata))
    # Create random permutation matrix to assign aggregators to different stratum in each round of the age
    permutation_matrix = create_permutation_matrix(Q, len(A))

    for t in range(0, T): # for each age
        for r in range(len(A)): # for each round
            client_models = [[] for x in range(len(A))]
            for q in range(Q): # for each stratum
                # Randomly select P clients from the stratum
                selected_clients = random.sample(strata[q], P)
                print('selected clients', selected_clients)

                # Server broadcasts model to clients
                model = A[permutation_matrix[q][r]]
                for client in selected_clients:
                    client_models[permutation_matrix[q][r]].append(client_update(model, opt[permutation_matrix[q][r]], client, num_epochs))
            # Aggregator updates the global model
            models = [aggregate_models(client_models[agg], A[agg], len(C)) for agg in range(len(A))]
        # Test after every age
        test_ensemble_model(models, test_dataloader)

    return models

def train(num_participants, num_models_in_ensemble, num_selected_participants, num_training_ages, Model, training_dataset,
                                   test_dataset, samples_per_batch, num_epoch, l_r, num_strata):
    # Create dataloaders
    train_dataloaders_clients = split_dataset(training_dataset, num_participants, samples_per_batch)
    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=samples_per_batch, shuffle=True)
    
    # Run Fed-Ensemble
    models_in_ensemble = list(itertools.repeat(Model(), num_models_in_ensemble))
    ensemble = fed_ensemble(train_dataloaders_clients, num_training_ages, models_in_ensemble, num_selected_participants, test_dataloader, l_r, num_epoch, num_strata)
    
    # Save all models
    for idx, aggregator in enumerate(ensemble):
    torch.save(aggregator.state_dict(), 'ensemble_model_{}.pth'.format(idx))
